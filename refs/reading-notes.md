# 参考文献读书笔记

## refs 目录论文清单

| # | 文件 | 论文 | 作者 | 年份/来源 |
|---|------|------|------|-----------|
| 1 | `MADDPG-Lowe2017.pdf` | Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments | Lowe, Wu, Tamar, Harb, Abbeel, Mordatch | NeurIPS 2017 |
| 2 | `DGPPO-Zhang2025.pdf` | Discrete GCBF Proximal Policy Optimization for Multi-agent Safe Optimal Control | Songyuan Zhang, Oswin So, Mitchell Black, Chuchu Fan | ICLR 2025 |
| 3 | `SmartDrones-ME4104A-Report.pdf` | Smart Collaborative Drones | Chong Yu Quan (NUS) | 毕设报告 2022 |
| 4 | `GCBF-Plus-Zhang2024.pdf` | GCBF+: A Neural Graph Control Barrier Function Framework | Zhang, So, et al. | arXiv 2024 |
| 5 | `RMP-original-Ratliff2018.pdf` | Riemannian Motion Policies | Ratliff, Issac, Kappler, Birchfield, Fox | arXiv 2018 |
| 6 | `RMPflow-Cheng2018.pdf` | RMPflow: A Computational Graph for Automatic Motion Policy Generation | Cheng, Mukadam, et al. | ISRR 2018 |
| 7 | `RMPflow-IEEE-Cheng2021.pdf` | RMPflow: A Geometric Framework for Generation of Multitask Motion Policies | Cheng, Mukadam, et al. | IEEE T-ASE 2021 |
| 8 | `MultiRobotRMP-Li2019.pdf` | Multi-Objective Policy Generation for Multi-Robot Systems Using Riemannian Motion Policies | Anqi Li, Mukadam, Egerstedt, Boots | arXiv 2019 |
| 9 | `FISOR-Zheng2024.pdf` | Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model | Yinan Zheng, Jianxiong Li, Dongjie Yu, Yujie Yang, Shengbo Eben Li, Xianyuan Zhan, Jingjing Liu | ICLR 2024 |
| 10 | `AFOR-Deng2025.pdf` | Coordinated Multi-Robot Navigation with Formation Adaptation | Zihao Deng, Peng Gao, Williard Joshua Jose, Christopher Reardon, Maggie Wigness, John Rogers, Hao Zhang | ICRA 2025 |
| 11 | `ATACOM-TRO-Liu2024.pdf` | Safe Reinforcement Learning on the Constraint Manifold: Theory and Applications | Puze Liu, Haitham Bou-Ammar, Jan Peters, Davide Tateo | IEEE T-RO 2024 |
| 12 | `MultiRobotFieldExplore-Lu2025.pdf` | RL-Based Dynamic Field Exploration & Reconstruction via Multi-Robot Systems | Thinh Lu, Divyam Sobti, Deepak Talwar, Wencen Wu | Frontiers in Robotics and AI 2025 |
| 13 | `RiemanLine-Li2025.pdf` | RiemanLine: Riemannian Manifold Representation of 3D Lines for Factor Graph Optimization | Yan Li, Ze Yang, Keisuke Tateno, Federico Tombari, Liang Zhao, Gim Hee Lee | AAAI 2026 (arXiv 2025) |
| 14 | `2406.13707_Safety-Critical-Formation-Control.pdf` | Safety-Critical Formation Control of Non-Holonomic Multi-Robot Systems in Communication-Limited Environments | Vishrut Bohara, Siavash Farzan | arXiv 2024 (under review) |
| 15 | `FormationPlanner-Cornwall2025.pdf` | Solving the Right Problem with Multi-Robot Formations | Chaz Cornwall, Jeremy P. Bos | arXiv 2025 |
| 16 | `SafeMARLFormation-Dawood2024.pdf` | Safe Multi-Agent Reinforcement Learning for Formation Control without Individual Reference Targets | Murad Dawood, Sicong Pan, Nils Dengler, Maren Bennewitz, Siqi Zhou, Angela P. Schoellig | IEEE RA-L 2025 |
| 17 | `DSP-GeometricMAPF-Atzmon2023.pdf` | Exploiting Geometric Constraints in Multi-Agent Pathfinding | Dor Atzmon, Sara Bernardini, Fabio Fagnani, David Fairbairn | ICAPS 2023 |

---

## 1. MADDPG (Lowe et al., NeurIPS 2017)

**链接**: [arxiv:1706.02275](https://arxiv.org/abs/1706.02275) | [GitHub](https://github.com/openai/maddpg)

### 核心思想

提出 **集中训练分散执行 (CTDE)** 范式下的多智能体 actor-critic 算法。

- **问题**：独立 Q-learning 时，其他智能体也在学习，环境对每个智能体来说是非稳态的，收敛性无法保证。独立策略梯度方差随智能体数量增长。
- **解法**：训练时 critic 看到所有智能体的观测和动作（集中化），但部署时 actor 仅使用自己的局部观测（分散化）。

### 框架

```
Actor_i:  pi_i(a_i | o_i)                    ← 仅看自己的观测
Critic_i: Q_i(x, a_1, a_2, ..., a_N)         ← 看所有人的状态+动作
```

- 基于 DDPG（确定性策略梯度），连续动作空间
- 每个智能体有自己的 critic（允许不同奖励函数，支持合作/竞争混合场景）
- 训练时使用 target 网络 + experience replay
- 可选：ensemble 训练（K 个子策略随机选择执行），增强鲁棒性

### 关键结果

| 任务 | MADDPG 表现 | vs 独立 DDPG |
|------|------------|-------------|
| 合作导航 (2 agent) | 更小目标距离，碰撞减半 | 显著优于 |
| 物理欺骗 | 94.4% 成功欺骗对手 | DDPG 仅 68.9% |
| 捕食者-猎物 | 碰撞率 16.1 | DDPG 仅 10.3 |
| 合作通信 | 84.0% 成功率 | DDPG 最高 32.0% |

### MPE 环境说明

2D 连续环境，智能体为圆形质点，连续观测（相对位置/速度），连续动作（2D 加速度）。简单物理（质量、碰撞）。后来被 PettingZoo、VMAS 继承和扩展。

### 与本课题的关系

MADDPG 的 CTDE 范式是 MAPPO/MACPO 等后续算法的基础。safe-po 中的 MAPPO、MACPO 都继承了这一架构。MADDPG 本身**不处理安全约束**。

---

## 2. DGPPO (Zhang et al., ICLR 2025)

**链接**: [arxiv:2502.03640](https://arxiv.org/abs/2502.03640) | [GitHub](https://github.com/MIT-REALM/dgppo) | [项目页](https://mit-realm.github.io/dgppo/)

### 解决的问题

分布式多智能体安全最优控制 (MASOCP)，同时面对 5 个挑战：
1. 未知离散时间动力学
2. 部分可观测
3. 动态变化的邻域
4. 输入约束
5. 无预设的高性能标称策略

### 核心方法：离散图控制屏障函数 (DGCBF) + PPO

**DCBF 定义**：函数 B: X -> R 满足 `B(x) <= 0 => inf_u [B(f(x,u)) - B(x) + alpha(B(x))] <= 0`，保证安全集 {x | B(x) <= 0} 是控制不变的。

**DGCBF 扩展**：在图结构上定义 CBF，通过图注意力机制自然处理邻域变化。

**关键理论发现**：约束值函数 V^{h,mu}（跟踪策略 mu 下的最大约束违反）自动是一个有效的 DCBF。因此学好值函数 = 获得免费的 CBF。

### 三大组件

| 组件 | 作用 |
|------|------|
| 通过值函数学习 DCBF | 不需手工设计屏障函数，值函数近似器自动产生安全证书 |
| 去掉标称策略依赖 | 直接约束学习策略，而非传统安全滤波 `min ||u - u_nom||^2 s.t. CBF` |
| Score function 梯度 | 不需动力学模型，仅用采样轨迹估计 CBF 条件梯度 |

### 算法流程

每次迭代执行**双重展开 (dual rollouts)**：
1. T 步随机策略展开 → 收集任务值函数 V^l 和策略梯度数据
2. T 步确定性策略展开 → 学习约束值函数 V^{h,mu}（即 DGCBF）

策略更新使用修改的伪优势函数：
```
A_tilde = A^{GAE} * 1[安全]  +  nu * max_m C_hat * 1[不安全]
```
安全时优化任务，不安全时切换到约束满足。

### 实验环境 (12 个，3 个仿真引擎)

**MPE 系列 (6 个)**: MPETarget, MPESpread, **MPEFormation**, **MPELine**, MPECorridor, MPEConnectSpread
**LiDAR 系列 (4 个)**: LidarTarget, LidarSpread, **LidarLine**, LidarBicycleTarget
**VMAS 系列 (2 个)**: VMASReverseTransport, VMASWheel

### 关键结果

- 安全率 ~99-100%，匹配最保守基线
- 任务性能匹配不考虑安全的基线 (MAPPO+GNN)
- **单一超参数集**跨所有 12 个环境，基线需要逐环境调参
- 可扩展到 N=3/5/7 智能体

### 与本课题的关系

DGPPO 和本论文的 ATACOM 流形约束方法是**平行的安全 MARL 方案**：

| 方面 | DGPPO | ATACOM (本论文) |
|------|-------|-----------------|
| 安全机制 | 图 CBF（学习的屏障函数） | 流形零空间投影（硬约束） |
| 几何结构 | 图上的屏障值函数 | 黎曼流形上的约束雅可比 |
| 动作修正 | CBF 约束下的策略优化 | 动作投影到约束切空间 |
| 保证强度 | 前向不变性（概率性） | 零空间投影（确定性） |

**互补方向**：DGPPO 的 GNN 通信机制可用于增强 ATACOM 的多智能体信息共享；ATACOM 的硬约束投影可替代 DGPPO 的软 CBF 约束。

---

## 3. Smart Collaborative Drones (Chong Yu Quan, NUS 毕设 2022)

**链接**: [GitHub](https://github.com/cyuquan8/smart_drones)

### 问题描述：区域防御任务

- x 架蓝色（防御方）无人机 vs y 架红色（进攻方）无人机
- 三个同心区域：监控区（外）→ 拦截区（中）→ 限制区（内，含高价值资产）
- 蓝方目标：在时间限制内阻止红方进入限制区
- 红方目标：在时间限制内突入限制区
- 基于 MPE 2D 环境，**稀疏共享奖励**（+10 成功 / -10 失败）

### 对比的算法

| 算法 | 类型 | 特点 |
|------|------|------|
| MADDPG | CTDE, off-policy | 标准基线 |
| **MADDPGv2** | CTDE, off-policy | 作者提出：MADDPG + UVFA + HER |
| MAPPO | CTDE, on-policy | PPO + 集中化值函数 |

三者都使用 GNN 增强的 critic 网络（GATv2 + GMT）。

### GATv2 + GMT 集成方式

**GATv2**（图注意力网络 v2）：
- 用于 critic 的图卷积层
- 输入：所有队友的局部观测作为全连接图的节点
- 动态注意力：`e(h_u, h_v) = a^T LeakyReLU([h_u || h_v] * W)`
- 3 层消息传递，1 个注意力头，128 维

**GMT**（图多集变换器）：
- 位于 GATv2 之上，将变长节点图表示池化为固定维向量
- 三阶段：GMPool_k（压缩为 k 个代表节点）→ SelfAtt（代表节点间交互）→ GMPool_1（输出单个图表示）
- 4 个注意力头，池化比 0.25

**数据流**：队友观测 → 图节点 → GATv2 → GMT → FFN → Q 值

### HER 如何工作

用于对抗稀疏奖励（蓝方正奖励极其稀少）：

- 每条轨迹不仅存储原始目标 g，还存储 k=4 个**事后目标**及重新计算的奖励
- 关键变体 **"goal distribution v2"**：根据胜率动态调整目标采样难度（隐式课程学习）
- 只适用于 off-policy 算法（MADDPG），因此 MAPPO 不能使用 HER

### 关键实验结果

| 配置 | 结果 |
|------|------|
| 1v1 | 红方（进攻方）全面压制蓝方 |
| 2v2 | 红方仍然压制，蓝方频繁出界 |
| **3v3** | **蓝方首次压制红方**，归因于碰撞频率足够高，为 HER 提供了正信号 |

核心发现：碰撞频率是蓝方能否学习的关键因素。3v3 时碰撞足够密集，HER 可以利用这些信号启动学习。

### 与本课题的关系

- 区域防御本质上是**约束下的多智能体空间协调**（限制区 = 安全约束区域）
- GATv2 的图注意力机制可与 RMPflow 结合：GNN 做高层任务分配，RMPflow/ATACOM 做低层安全运动生成
- 稀疏奖励 + HER 的思路对 safe-po 的多智能体训练有参考价值

---

## 4. FISOR (Zheng et al., ICLR 2024)

**链接**: [arxiv:2401.10700](https://arxiv.org/abs/2401.10700) | [GitHub](https://github.com/ZhengYinan-AIR/FISOR) | [项目页](https://zhengyinan-air.github.io/FISOR/)

### 解决的问题

安全离线强化学习 (Safe Offline RL) 中的**硬约束满足**问题。

现有方法的三个核心局限：
1. **软约束不够安全**：所有先前方法仅约束期望累积代价 `E[Σγ^t c(s_t)] ≤ l`，允许单步违反约束，在安全关键场景不可接受
2. **三目标耦合**：安全约束满足、奖励最大化、离线行为正则化三个目标紧密耦合，联合优化不稳定
3. **代价限制需调参**：软约束的阈值 l 需逐任务调节，最优值因任务而异

### 核心思想：可行性引导的解耦

**关键洞察**：通过安全控制理论中的可达性分析 (HJ Reachability)，硬安全约束可以等价转化为**识别最大可行域**的问题。

这使得原来的三元耦合问题分解为三个**解耦的监督学习目标**：

```
原问题: max_π E[V_r^π]  s.t.  V_c^π ≤ 0,  D(π||π_β) ≤ ε
                    ↓ 可行性分解
可行域 S_f 内: max_π E[V_r^π]     s.t.  π ∈ Π_f(s),  KL(π||π_β) ≤ ε
不可行域外:   max_π E[-V_h^π]    s.t.  KL(π||π_β) ≤ ε
```

### 三个解耦学习过程

| 步骤 | 目标 | 方法 |
|------|------|------|
| 1. 可行域识别 | 离线学习最优可行值函数 V_h* | 反向 expectile 回归 (Eq. 8-9) |
| 2. 优势函数学习 | 学习 A_r* (奖励优势) | IQL in-sample learning (Eq. 15-16) |
| 3. 策略提取 | 从数据集提取最优策略 | 加权扩散模型 (Eq. 14) |

### Hamilton-Jacobi 可达性分析

**最优可行值函数**定义 (Definition 1)：

```
V_h*(s) = min_π max_{t∈N} h(s_t),   s_0 = s, a_t ~ π(·|s_t)
```

含义：从状态 s 出发，最优策略下轨迹中**最大约束违反值**。

关键性质：
- `V_h*(s) ≤ 0` ⇒ 存在策略从 s 出发永不违反约束（**可行状态**）
- `V_h*(s) > 0` ⇒ 任何策略从 s 出发都会违反约束（**不可行状态**）

**可行域**: `S_f* = {s | V_h*(s) ≤ 0}` — 所有可行状态的集合

**离线学习**：用可行 Bellman 算子 + 反向 expectile 回归从离线数据中近似 V_h*，避免在线交互。

### 加权行为克隆形式的最优策略

**定理 1** (闭式解)：

```
π*(a|s) ∝ π_β(a|s) · w(s, a)
```

权重函数根据可行性区分：

```
w(s, a) = {
    exp(α₁ · A_r*(s,a)) · I[Q_h*(s,a)≤0]    若 V_h*(s) ≤ 0  (可行域)
    exp(-α₂ · A_h*(s,a))                      若 V_h*(s) > 0  (不可行域)
}
```

- **可行域**：选择高奖励优势的动作，同时确保不离开可行域 (`Q_h*(s,a)≤0`)
- **不可行域**：选择最小化约束违反的动作（恢复行为）

### 扩散模型策略提取

使用扩散模型参数化策略（而非高斯），因其强大的多模态分布建模能力。

**定理 2** (加权回归 = 精确能量引导)：直接将权重 w(s,a) 乘到扩散训练损失中：

```
min_θ E_{t,z,(s,a)~D} [w(s,a) · ||z - z_θ(a_t, s, t)||²]
```

**无需训练时间依赖分类器**，大幅简化了引导扩散采样，相比 TREBI 等方法训练更简单稳定。

推理时额外采样 N=16 个候选动作，选择 Q_h* 最小的那个（最安全），进一步增强安全性。

### 实验结果 (DSRL 基准)

26 个任务横跨三个仿真平台：

| 平台 | 任务 |
|------|------|
| Safety-Gymnasium (9个) | CarButton1/2, CarPush1/2, CarGoal1/2, AntVel, HalfCheetahVel, SwimmerVel |
| Bullet-Safety-Gym (8个) | Ant/Ball/Car/Drone × Run/Circle |
| MetaDrive (9个) | easy/medium/hard × sparse/mean/dense |

**核心结果**：

| 方法 | Safety-Gym 安全率 | Bullet-Gym 安全率 | MetaDrive 安全率 | 特点 |
|------|------------------|------------------|-----------------|------|
| **FISOR** | **全部安全** | **全部安全** | **全部安全** | 唯一全场景安全 |
| BCQ-Lag | 部分安全 | 部分安全 | 部分安全 | 拉格朗日不稳定 |
| CPQ | 安全但奖励极低 | 部分安全 | 全部不安全 | 过度保守 |
| COptiDICE | 部分安全 | 部分安全 | 部分安全 | DICE 残差学习不稳定 |
| CDT | 部分安全 | 安全 | 部分安全 | 计算昂贵 |
| TREBI | 部分安全 | 安全 | 部分安全 | 需时间依赖分类器 |

**FISOR 是唯一在全部 26 个任务上都满足安全约束的方法，同时在大多数任务上获得最高奖励。**

### 消融实验关键发现

| 变体 | 影响 |
|------|------|
| w/o HJ (用代价值函数替代可行值函数) | 可行域估计不准确 → 策略过度保守 → 回报大幅下降 |
| w/o infeasible (去掉不可行域目标) | 不可行状态下无恢复行为 → 约束违反严重 (3-6x) |
| w/o diffusion (用高斯策略替代扩散) | 多模态分布拟合差 → 安全性下降 |
| 超参数 τ, N | 主要影响保守程度，不显著影响安全性；τ=0.9, N=16 跨任务通用 |

### 方法定位：硬约束 vs 软约束

```
安全 RL 约束谱系：

软约束方法 (期望约束)                                硬约束方法 (逐状态约束)
│                                                    │
├─ 拉格朗日法: PPO-Lag, MACPO, BCQ-Lag              ├─ CBF: DGPPO, GCBF+
├─ 信赖域法: CPO, PCPO                              ├─ HJ 可达性: RCRL (在线), FISOR (离线)
├─ DICE 法: COptiDICE                               └─ 流形投影: ATACOM (本论文)
└─ 条件生成: CDT, TREBI
```

### 与本课题的关系

FISOR 与本课题 ATACOM 方法在**安全机制层面是平行但互补的**：

| 方面 | ATACOM (本论文) | FISOR |
|------|----------------|-------|
| 设置 | 在线多智能体 | 离线单智能体 |
| 安全类型 | 硬约束（零空间投影） | 硬约束（可行域识别） |
| 约束表示 | 约束雅可比矩阵 J_c | 可行值函数 V_h* |
| 安全执行点 | 动作空间投影（env.step 前） | 策略学习阶段（加权 BC） |
| 动力学要求 | 需要（计算雅可比） | 不需要（纯离线数据） |
| 策略表示 | 高斯（PPO actor） | 扩散模型（多模态） |

**三个互补方向**：

1. **可行域概念**：FISOR 的可行域 `{s | V_h*(s) ≤ 0}` 可为 ATACOM 提供状态空间层面的安全判据。ATACOM 当前仅在动作空间做投影，缺乏状态可行性判断 — 如果当前状态已不可行，ATACOM 的投影无法保证恢复安全。FISOR 的可行域可以为 ATACOM 添加"紧急模式"。

2. **离线预训练 → 在线微调**：FISOR 的离线策略可作为 ATACOM 在线训练的初始化策略，避免在线探索初期的大量约束违反。FISOR 论文也提出 offline-to-online 作为未来方向。

3. **扩散模型策略**：FISOR 使用扩散模型参数化策略，天然适合多模态行为分布。这可能在编队任务中有价值 — 多个等价编队构型对应多个模态，高斯策略容易模式坍缩。

---

## 5. AFOR (Deng et al., ICRA 2025)

**链接**: [arxiv:2404.01618](https://arxiv.org/abs/2404.01618) | [项目页](https://hcrlab.gitlab.io/project/afor/)

### 解决的问题

多机器人**编队自适应导航** — 机器人团队在复杂环境（特别是窄通道）中需要动态调整编队形状，同时保持协调导航。

现有方法的局限：
1. **Leader-Follower (L&F)**：刚性编队，无法通过窄走廊；高度依赖 leader
2. **Decentralized GNN (DGNN)**：能到达目标但不维护编队形状
3. **优化方法**：计算量大，实时响应慢
4. **单层 RL**：容易不收敛

### 核心方法：AFOR（自适应编队 + 振荡抑制）

**分层学习框架**，将编队控制分为上下两层：

```
┌─────────────────────────────────────┐
│  上层：GNN 团队协调                    │
│  · 图神经网络 (radius graph)          │
│  · 节点 = 机器人状态 [pos, vel, goal] │
│  · 消息传递: h_i = W·z_i + Σ W·(z_j - z_i)  │
│  · 输出: 团队级状态嵌入 h_i           │
├─────────────────────────────────────┤
│  弹簧-阻尼器模型 (编队自适应)          │
│  · 弹簧: 维持期望距离 |d_ij - p_ij|  │
│  · 阻尼器: 抑制振荡 |q_i - q_j|      │
│  · R_adapt = Σ -α|d-p| - (1-α)|Δq|  │
├─────────────────────────────────────┤
│  下层：RL 个体控制                     │
│  · 输入: h_i (来自上层)               │
│  · 输出: a_i = [vx, vy] 速度指令      │
│  · 策略 π_θ(a_i|s_i) 学习导航+避障    │
└─────────────────────────────────────┘
          ↕ PPO 统一训练 ↕
```

### GNN 消息传递机制

```python
# 状态编码
z_i = MLP(s_i)              # s_i = [pos, vel, goal, obstacle_dist]

# 图神经网络消息传递
h_i = W_h · z_i + Σ_{j∈N(i)} W_h · (z_j - z_i)

# 动作生成
a_i = φ(h_i)                # 两层线性 + ReLU
```

关键设计：消息传递中使用**状态差 (z_j - z_i)** 而非拼接，直接编码了邻居的相对信息，这对编队保持非常自然。

### 弹簧-阻尼器奖励

```
R_adapt = Σ_{v_i,v_j ∈ V} [-α|d_ij - p_ij| - (1-α)·q_ij]
```

- **弹簧分量** `|d_ij - p_ij|`：惩罚实际距离偏离期望距离（编队保持）
- **阻尼器分量** `q_ij = |q_i - q_j|`：惩罚相对速度差（振荡抑制）
- α 平衡两者，配合目标到达奖励和避障奖励共同训练

### 评估指标

**CFI (Contextual Formation Integrity)**：综合评价编队完整度

```
CFI = β · [1 - δ⁻¹ · min(|W-(τ+δ)|, |W-(τ-δ)|)] + (1-β) · ε
```

- W = 编队最大宽度，τ = 走廊安全阈值，δ = 编队刚度参数
- ε ∈ [0,1] = 编队形状完整度
- CFI ∈ [0,1]，越高越好

### 实验结果

**Gazebo 仿真 (5 个机器人，3 种编队)**：

| 方法 | Circle SR | Circle CFI(δ<0.1) | Wedge SR | Wedge CFI(δ<0.1) | Line SR | Line CFI(δ<0.1) |
|------|----------|------------------|----------|-----------------|---------|-----------------|
| L&F | 80% | 70.1 | 60% | 66.7 | 60% | 55.6 |
| DGNN | 100% | 58.9 | 60% | 42.3 | 100% | 20.2 |
| Baseline (无弹簧阻尼) | 100% | 80.5 | 100% | 68.2 | 100% | 64.3 |
| **AFOR** | **100%** | **90.4** | **100%** | **90.4** | **100%** | **85.1** |

核心发现：
- L&F 在窄通道卡住（刚性编队无法适应）
- DGNN 能到达目标但编队完全散乱（CFI 低至 20%）
- 弹簧-阻尼器模型贡献 10-20% 的 CFI 提升（对比 Baseline）
- **AFOR 在所有编队类型和不确定度水平下均最优**

**可扩展性 (3/5/7/9 个机器人)**：
- 3 机器人 CFI > 90%
- 9 机器人 CFI ≥ 80%
- 随团队规模增大 CFI 略降但稳定

**真实机器人验证**：5 个全向轮式机器人，成功在窄走廊中自适应编队。

### 方法特点

| 特性 | AFOR |
|------|------|
| 控制架构 | 分层 (GNN + RL) |
| 编队维持 | 弹簧模型（奖励函数） |
| 振荡抑制 | 阻尼器模型（奖励函数） |
| 编队适应 | 窄通道时自动压缩编队 |
| 训练方式 | PPO 端到端训练两层 |
| 执行方式 | 集中式 (centralized execution) |
| 编队类型 | Circle, Wedge, Line |
| 安全约束 | 无显式安全机制（靠避障奖励） |

### 与本课题的关系

AFOR 与本论文的 ATACOM 方法在**编队控制层面高度互补**：

| 方面 | AFOR | ATACOM (本论文) |
|------|------|----------------|
| 编队维持 | 弹簧奖励（软约束） | 双边距离约束雅可比（硬约束） |
| 碰撞避免 | RL 学习的避障行为 | 零空间投影保证 |
| 编队适应 | 弹簧自然允许变形 | 约束松弛变量 (slack) 允许 |
| 安全保证 | 无（靠训练质量） | 确定性（雅可比投影） |
| 通信 | GNN 消息传递 | 无（独立投影） |
| 执行 | 集中式 | 分散式 |

**三个融合方向**：

1. **AFOR 的分层结构 + ATACOM 的安全层**：用 GNN 上层做团队协调和编队规划，ATACOM 在下层做安全动作投影。这相当于给 AFOR 加上硬安全保证。

2. **弹簧-阻尼器作为 ATACOM 的编队约束**：将 AFOR 的弹簧模型 `|d_ij - p_ij|` 转化为 ATACOM 的不等式约束 `d_min ≤ ||p_i - p_j|| ≤ d_max`，从奖励工程变为约束雅可比投影，获得编队保持的硬保证。

3. **GNN 替代独立投影**：当前 ATACOM 对每个智能体独立做约束投影（不感知其他智能体的意图），AFOR 的 GNN 消息传递可以让每个智能体在投影前先获得团队信息，做出更协调的动作。

---

## 6. ATACOM — Safe RL on the Constraint Manifold (Liu et al., IEEE T-RO 2024)

**链接**: [arxiv:2404.09080](https://arxiv.org/abs/2404.09080) | [项目页](https://puzeliu.github.io/TRO-ATACOM) | [IEEE Xplore](https://ieeexplore.ieee.org/document/10989544/)

### 核心思想

**ATACOM (Acting on the TAngent Space of the COnstraint Manifold)** — 将安全约束表面建模为**约束流形**，在流形的**切空间**上构建安全动作空间，使 RL 智能体采样的任何动作都自动满足安全约束。

```
传统安全 RL 方法:  π(a|s) → 安全滤波器 → a_safe    (事后修正)
ATACOM 方法:       π(u|s) → 切空间映射 → a_safe    (构造性安全)
```

### 数学框架

**1. 约束流形定义**

给定不等式约束 `k(s) ≤ 0`，引入松弛变量 `µ ≥ 0`，将不等式转为等式：

```
c(s, µ) = k(s) + µ = 0
```

安全集 `M = {(s, µ) ∈ D : c(s, µ) = 0}` 是 N 维扩展状态空间中的 S 维嵌入子流形（Constant-Rank Level Set 定理）。

**2. 切空间安全动作**

在流形上任意一点 (s, µ) ∈ M，切空间为：

```
T_{(s,µ)} M = ker J_c(s, µ) = {v ∈ R^N : J_c · v = 0}
```

沿切方向运动保证系统**始终停留在流形上**（即安全集内）。

**3. ATACOM 安全控制器**

```
[u_s; u_µ] = -J_u† · ψ  -  λ · J_u† · c  +  B_u · u
               ↑漂移补偿      ↑收缩项          ↑切向项(RL动作)
```

其中：
- `J_u = [J_k·G, A(µ)]`：约束雅可比
- `ψ = J_k·f`：系统漂移引起的约束漂移
- `B_u`：切空间基（满足 `J_u·B_u = 0`）
- `u`：RL 智能体输出的**安全动作**（任意值均安全）
- `λ > 0`：收缩增益

### 安全性证明（定理 3）

使用 **LaSalle 不变集原理**证明：

Lyapunov 函数: `V(s,µ) = ½ c^T c`

```
V̇ = -λ c^T J_u J_u† c ≤ 0
```

在吸引域 Ω_η 内（排除奇异集），系统轨迹收敛到约束流形 M（正向不变性）。

扩展到**随机策略** (RL 设置)：将系统视为切换系统，用 Theorem 4 证明安全性对所有采样动作成立。

### 鲁棒性分析（定理 5）

存在模型失配/扰动 `ε` 时，系统是**输入-状态稳定 (ISS)** 的：

```
||c|| ≤ η_c = ω·η_J / λ
```

约束违反量被有界扰动 ω 和增益 λ 所限定。增大 λ 可缩小违反上界。

### 六大贡献

| # | 贡献 | 技术细节 |
|---|------|---------|
| 1 | 松弛变量动态模型 | `µ̇ = α(µ)·u_µ`，α 为 class-K 函数，保证 µ > 0 |
| 2 | 扰动下 ISS 分析 | 有界扰动 → 有界约束违反，可量化 |
| 3 | 光滑切空间基 | Orthogonal Procrustes 问题 → 连续变化的正交基（Alg. 3） |
| 4 | 松弛动态选型 | 线性 α=βµ vs 指数 α=exp(βµ)-1，指数型远离边界时变形更小 |
| 5 | 漂移裁剪 | 仅补偿趋向边界的漂移 `ψ̂_i = max(ψ_i, 0)`，减少不必要修正 |
| 6 | 多场景扩展 | 二阶动力学、动态环境（不可控状态）、等式约束 |

### 实验验证

**1. 2D 碰撞避免** — ATACOM vs CBF-QP：

| 方面 | ATACOM | CBF-OSQP | CBF-CvxpyLayer |
|------|--------|----------|----------------|
| 安全率 | 99.6% | 94.7% | 94.6% |
| 奖励 | 157.0 | 151.9 | 144.7 |
| 计算时间 | 0.47 ms | 2.65 ms | - |
| 抗扰动 | 强（ISS 保证） | 弱 | 弱 |

ATACOM 安全更高、更快、更鲁棒。原因：ATACOM 在接近边界时**提前变形**动作空间，QP 方法**事后修正**。

**2. 机器人气球曲棍球 (Air Hockey)**：
- 7-DOF KUKA iiwa14 机械臂，33 维观测，7 维动作
- 关节空间 14 个约束 + 笛卡尔空间 7 个约束 = **21 个安全约束**
- 模拟训练后**真实机器人在线微调**（首次 ATACOM 实机在线学习）
- 动态失配实验：标称模型速度 50%~150%，ATACOM 均保持安全
- 实机实验：3000 回合在线微调后，成功率从 12% 提升至 ~60%，约束违反率 < 0.1%

### 与本课题的直接关系

**这篇论文就是本课题 `mult_cm.py` 的理论基础。** `AtacomEnvWrapper` 和 `MultiNavAtacom` 直接实现了本文的方法。

代码-论文对应关系：

| 论文公式 | 代码位置 |
|---------|---------|
| `c(s,µ) = k(s) + µ` | `constraints.c(q, x, origin_constr=False)` |
| `J_u = [J_k·G, A(µ)]` | `_construct_Jc()` → `np.hstack([J_q, J_s])` |
| `B_u = ker(J_u)` | `Nc = (I - J_c†·J_c)[:, :dim_null]` |
| `-J_u†·ψ` (漂移补偿) | `dq_uncontrol = -J_c†·J_x·dx` |
| `-λ·J_u†·c` (收缩项) | `dq_err = -K_c·(J_c†·c)` |
| `B_u·u` (切向项) | `dq_null = Nc·α` |
| 安全控制输出 | `dq = dq_null + dq_err + dq_uncontrol` |

**当前代码未实现的论文内容**：
- 光滑切空间基（Alg. 3）— 当前用简单投影
- 漂移裁剪 — 当前全量补偿
- 二阶动力学扩展 — Safety-Gymnasium 的 Point robot 是速度控制，暂不需要
- ISS 分析的量化违反上界 — 可用于调参 λ (即 K_c)

---

## 7. Multi-Robot Field Exploration & Reconstruction (Lu et al., Frontiers 2025)

**链接**: [DOI:10.3389/frobt.2025.1492526](https://doi.org/10.3389/frobt.2025.1492526) | [PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11975907/)

### 解决的问题

多机器人编队在**动态扩散场**（如野火、气体泄漏、水污染）中执行**探索与重建**任务：
- 多个非重叠扩散源，各有不同扩散系数和对流系数
- 机器人编队需在时间约束内发现所有源并重建浓度场
- 兼顾探索速度与重建精度

### 核心方法：双模式策略

```
┌──────────────────────────────────┐
│  Source Mapping 模式（PPO 引导）  │
│  · 浓度梯度追踪 → 定位扩散源中心  │
│  · 达到稳态 → 估计对流系数       │
│  · CKF + RLS 在线参数辨识        │
├──────────────────────────────────┤
│  Field Exploration 模式          │
│  · K-means 聚类划分未探索区域     │
│  · 贪心访问最近聚类中心          │
│  · A* 路径规划移向目标           │
│  · 检测高浓度时切换到 Source Map  │
└──────────────────────────────────┘
     ↕ 浓度阈值 δ 触发模式切换 ↕
```

### PPO Source Mapping

**观测状态**：`S(k) = [z(r_c, k), ∇z_x(r_c, k), ∇z_y(r_c, k)]`
- 编队中心处的浓度估计 + xy 方向浓度梯度（来自 CKF）

**离散动作空间**：9 个方向（8 方向 + 停留）

**奖励函数**：`R(k) = α · Σ_{r∈Γ(k)} z(r, k)`
- 视野范围内浓度总和 → 激励编队向高浓度区域移动

**训练策略**：课程学习
1. 阶段 1：静态场（无对流）预训练 40 万步
2. 阶段 2：动态场（随机对流系数）训练 200 万步

**稳态判断**：`z(r_c, k) > z(r_c, k-1)` 且 `z(r_c, k) > z(r_c, k+1)` 且 `∇z(r_c, k) ≈ 0`

### K-means Field Exploration

```python
K = max(2, floor(S_map / S̃_field))   # 基于场大小估计的聚类数

# 发现新源后更新 K:
S'_field = (1-β)·S̃_field + β·S_new   # 指数移动平均
S_new ≈ π · R²_dist                    # 新源大小估计
```

- 将未探索区域用 K-means 聚类
- 贪心选择最近聚类中心作为下一个导航目标
- 用 A* 路径规划到达目标
- 发现新源后重新划分（K 递减）

### 协作卡尔曼滤波 (CKF) + RLS

- **CKF**：4 个机器人对称编队，融合各机器人传感器测量，估计编队中心的浓度和梯度
- **RLS**：利用 CKF 输出在线辨识扩散系数 θ
- **对流估计**：在稳态时，编队速度 = 对流速度 → `v̂ = Δr_c / T`
- **场重建**：离散化扩散方程（有限差分）在已知参数下前向传播

### 实验结果

**仿真 (100×100 网格，15 个环境)**：

| 方法 | 检测速度 | 重建误差 | 全源检测率 |
|------|---------|---------|-----------|
| Random Walking | 慢 | 高 | 低 |
| Lawn Mowing | 中 | 中 | 中 |
| **PPO + K-means (Ours)** | **快** | **低** | **全部检测** |

在 k=300 步内，本方法是唯一能在所有环境中检测到全部扩散源的方法。

**实机实验 (4 个差分驱动机器人)**：
- 12×12 英尺实验场地，投影仿真扩散场
- ROS Noetic + 运动捕捉系统 + CKF
- 2 源和 3 源场景均成功探索并重建

### 与本课题的关系

本文展示了**编队 + RL 路径规划**在环境监测中的应用，与 ATACOM 的融合方向：

| 方面 | 本文 (Lu 2025) | ATACOM (本论文) |
|------|---------------|----------------|
| 编队控制 | Jacobi 变换解耦（经典方法） | 约束雅可比零空间投影 |
| 安全机制 | 无（靠编队控制器维持） | 流形投影保证 |
| RL 用途 | 路径规划（离散方向选择） | 底层安全运动控制 |
| 任务 | 扩散源探索与重建 | 安全导航/编队保持 |

**三个融合方向**：

1. **ATACOM 保护编队**：本文的编队在探索过程中无安全保证（可能碰障碍物）。将 ATACOM 的约束投影加在编队控制器底层，保证避障 + 编队保持。

2. **PPO + K-means 作为高层任务分配**：本文的双模式策略可作为 ATACOM 多机器人系统的**任务规划层** — PPO 决定"去哪里"，ATACOM 决定"怎么安全到达"。

3. **CKF 的信息共享**：CKF 利用多机器人测量融合估计环境状态。这种协作感知机制可以增强 ATACOM 多智能体中当前缺失的**信息共享**能力 — 如用 CKF 估计其他智能体的意图/状态，提供给 ATACOM 的约束函数。

---

## 8. RiemanLine (Li et al., AAAI 2026)

**链接**: [arxiv:2508.04335](https://arxiv.org/abs/2508.04335) | [GitHub](https://github.com/yanyan-li/RiemanLine)

### 解决的问题

SLAM/视觉里程计中 **3D 直线地标的最小化参数表示** 问题。

现有方法的局限：
1. **正交表示 (Orthonormal)**：每条线 4 自由度，n 条平行线需 4n 参数，无法紧凑编码结构性规律
2. **独立处理**：所有线特征独立优化，忽视人造环境中普遍存在的平行线结构信息
3. **额外约束**：编码平行关系需在损失函数中手动添加约束项，增加复杂度和参数量
4. **Manhattan/Atlanta World 假设**：过于严格，无法处理一般环境

### 核心思想：黎曼流形上的解耦表示

将每条 3D 直线**解耦为全局分量和局部分量**，分别在不同黎曼流形上优化：

```
3D 直线 L = (u₂, ω·u₁)
              ↓           ↓
     全局：方向向量      局部：缩放法向量
     u₂ ∈ S²            ω·u₁ ∈ λS¹
     (单位球面, 2 DoF)   (缩放圆, 2 DoF)
              ↓           ↓
     切空间优化:         切空间优化:
     T_{u₂}S² = {x: x^T u₂=0}   T_{u₁}λS¹ = {x: x^T u₁=0, x^T u₂=0}
     δθ = [δθ₁, δθ₂]    δγ (角度扰动) + λ (距离尺度)
```

### S² 球面上的方向优化

方向向量 u₂ (||u₂||=1) 位于单位球面 S² 上。切空间 T_{u₂}S² 是 2 维线性空间：

```
T_{u₂}S² = {x ∈ R³ : x^T u₂ = 0}
```

扰动通过黎曼指数映射回球面：

```
v₂ = u₂ ⊞ δm = u₂·cos||δm|| + (δm/||δm||)·sin||δm||    (Eq. 5)
```

其中 δm = [b_x, b_y]·[δθ₁, δθ₂]^T，b_x, b_y 是切空间正交基。

### λS¹ 缩放圆上的法向量优化

缩放法向量 ω·u₁ 位于与 u₂ 正交的平面上的缩放圆 λS¹。切空间仅 1 维：

```
T_{u₁}λS¹ = {x ∈ R³ : x^T u₁ = 0, x^T u₂ = 0}
```

更新通过 Rodrigues 旋转公式：

```
v₁ = λ·cos(δγ)·u₁ + sin(δγ)·u₃    (Eq. 7)
```

其中 u₃ = v₂ × u₁，λ ∈ R⁺ 为距离尺度。

### 平行线统一表示 (StructRiemanLine)

n 条平行线共享方向 u₂，各有独立法向量：

```
平行线组 S = [u₂, ω⁰u₁⁰, ω¹u₁¹, ..., ωᵏu₁ᵏ]

参数空间: [δθ, δγ⁰, ..., δγᵏ, λ⁰, ..., λᵏ]
         2 DoF + k×1 DoF + k×1 DoF = 2 + 2k
```

**参数量对比**：

| 表示方法 | n 条平行线参数量 | 20 条平行线 |
|---------|----------------|------------|
| Orthonormal | 4n | 80 |
| **StructRiemanLine** | **2n + 2** | **42** |

减少约 47.5% 的参数量，且**隐式编码平行性**，无需额外约束项。

### 因子图框架

```
因子图 G = (V, E)

顶点:
  · V_pose: 相机位姿 T_{w,c} ∈ SE(3)
  · V_p: 点地标 P^w ∈ R³
  · V_l: 线地标 (RiemanLine 参数化)
  · V_para: 平行线组 (共享方向)

因子 (边):
  · 点共视因子: r_p(p̄, P^w, T_{w,c})          — 点重投影误差
  · 线共视因子: r_l(p̄_s, p̄_e, L^w, T_{w,c})  — 端点到重投影线的距离 (Eq. 10)
  · 平行约束因子: r_∥ = (1/(N-1))·Σ(1 - u_i^T u_j)  — 方向一致性 (Eq. 11)
```

**StructRiemanLine 的优势**：平行线共享 u₂ 顶点，平行约束**内嵌于参数化本身**，无需额外约束因子。而 OrthLine+Constraint 需要独立参数 + 显式约束边。

### 实验结果

**1. ICL-NUIM 基准 (8 个室内 RGB-D 序列)**

| 方法 | livingroom2 RMSE (cm) | office2 RMSE (cm) | 整体表现 |
|------|----------------------|-------------------|---------|
| Initial Factor Graph | 2.15 | 2.95 | 高漂移 |
| Point OrthLine | 0.79 | 0.75 | 良好 |
| Point RiemanLine | 0.78 | 0.75 | 良好 |
| OrthLine Constr | 0.79 | 0.76 | 良好 |
| **StructRiemanLine** | **0.75** | **0.75** | **最优** |

**2. TartanAir (4 个合成序列)** — 结构丰富的室内场景提升最大

| 方法 | Hospital 平移 (cm) | Hospital 旋转 (°) | Office 平移 (cm) |
|------|-------------------|------------------|-----------------|
| Initial | 33.78 | 26.83 | 57.00 |
| RiemanLine | 7.29 | 6.60 | 40.95 |
| OrthLine Constr | 5.49 | 4.97 | 50.04 |
| **StructRiemanLine** | **2.91** | **2.04** | **15.60** |

Hospital 序列平移误差从 33.78→2.91 cm（降 91%），旋转误差从 26.83°→2.04°（降 92%）。

**3. 直线重建质量 (TartanAir)**

StructRiemanLine 在 Hospital 序列中方向误差 1.60°，法向误差 0.91°。超过 90% 的重建直线在方向和法向上误差 < 2°。

**4. 计算效率 (box 仿真)**

| 方法 | 参数块 | 有效参数 | 优化时间 (s) |
|------|-------|---------|-------------|
| OrthLine Constr | 597 | 3152 | 97.62 |
| **StructRiemanLine** | **579** | **3116** | **49.62** |

**约 49% 的速度提升**，归因于更紧凑的参数化产生更稀疏、条件数更好的 Hessian 矩阵。

### 关键技术总结

| 技术 | 说明 |
|------|------|
| S² 流形优化 | 方向向量在单位球面切空间上优化，保证归一化 |
| λS¹ 流形优化 | 缩放法向量在正交缩放圆上优化，解耦距离和角度 |
| 解耦表示 | 全局方向 + 局部法向量，自然编码结构规律 |
| 2n+2 最小参数化 | n 条平行线比 Orthonormal(4n) 减少约 50% 参数 |
| 统一因子图 | 共视因子 + 结构因子在同一流形优化框架中 |

### 与本课题的关系

RiemanLine 与本课题在**黎曼流形的数学工具层面高度相关**，但应用领域完全不同：

| 方面 | RiemanLine | ATACOM (本论文) |
|------|-----------|----------------|
| 领域 | 视觉 SLAM / 因子图优化 | 安全强化学习 |
| 流形对象 | 3D 直线地标 (S² × λS¹) | 安全约束集 M = {c(s,µ)=0} |
| 切空间用途 | 参数更新的搜索方向 | 安全动作的投影空间 |
| 指数映射 | 将更新映射回球面 | 沿流形运动保持安全 |
| 零空间 | 无（直接在切空间优化） | ker(J_c) 构成安全动作空间 |
| 雅可比矩阵 | 重投影雅可比（因子图残差） | 约束雅可比 J_u（安全投影） |

**共享的数学基础**：

```
切空间:
  RiemanLine:  T_{u₂}S² = {x: x^T u₂ = 0}
  ATACOM:      T_{(s,µ)}M = ker J_c(s,µ) = {v: J_c·v = 0}
  → 都是通过内积为零定义正交补空间

指数映射:
  RiemanLine:  v₂ = Exp_{u₂}(δm) — 球面上的测地线
  ATACOM:      沿流形运动 = 切向分量 B_u·u — 近似流形上的运动

参数最小化:
  RiemanLine:  4n → 2n+2（利用共享方向）
  ATACOM:      高维动作空间 → dim(ker J_c) 维安全动作（利用约束结构）
```

**启发方向**：

1. **切空间基的光滑性**：RiemanLine 通过 Rodrigues 公式保证切空间基的连续变化。ATACOM-TRO 论文提出了 Orthogonal Procrustes 方法（Alg. 3）解决类似问题，但 safe-po 代码尚未实现。RiemanLine 的方法可作为参考。

2. **结构化约束的最小参数化**：RiemanLine 将 n 条平行线的 4n 参数降为 2n+2。类似思路可用于 ATACOM 多智能体：n 个智能体的碰撞避免约束可利用对称性结构化表示，减少约束雅可比维度。

3. **因子图 ↔ 约束图**：RiemanLine 的因子图连接相机位姿和地标；ATACOM 多智能体中约束函数连接各智能体状态。两者都涉及稀疏结构上的联合优化，RiemanLine 的稀疏求解策略对大规模多智能体 ATACOM 有参考价值。

---

## 论文间的关系图

```
RMP (Ratliff 2018)
    ↓ 计算图扩展
RMPflow (Cheng 2018/2021)
    ↓ 多机器人扩展
Multi-Robot RMP (Li 2019)  ←─── 本仓库 multi-robot-rmpflow 代码
    ↓ 几何思想迁移                     ↑ 理论基础
ATACOM / safe-po            ←───── ATACOM-TRO (Liu 2024)
  ← 论文核心贡献（在线硬约束）          约束流形 + 切空间理论
    ↑              ↑            ↑              ↑             ↑
    │ 安全 MARL    │ 训练框架   │ 可行域        │ 编队控制    │ 编队+探索
    │ 对比方法     │ 基础       │               │             │
DGPPO (2025)   MADDPG (2017)  FISOR (2024)  AFOR (2025)  FieldExplore
    │                ↓              ↑             │         (Lu 2025)
    │           MADDPGv2+GNN        │ HJ 可达性    │ GNN      │ PPO+K-means
    │           Smart Drones        │             │ 弹簧阻尼  │ CKF+RLS
    ↓                               ↓             ↓          ↓
  GCBF+ (2024)             安全离线 RL      编队自适应    场重建

                    ·· 黎曼流形数学工具 ··
                    ↑                    ↑
          ATACOM-TRO (Liu 2024)    RiemanLine (Li 2025)
          约束流形 M 上的切空间      S²×λS¹ 上的直线参数化
          安全动作投影              因子图优化
          → 安全 RL               → 视觉 SLAM
```

---

## 四种安全 RL 方案对比

| 方面 | ATACOM (本论文) | DGPPO (ICLR 2025) | MACPO (safe-po 基线) | FISOR (ICLR 2024) |
|------|----------------|-------------------|---------------------|-------------------|
| 安全机制 | 黎曼流形零空间投影 | 离散图控制屏障函数 | 信赖域 + 拉格朗日对偶 | HJ 可达性 + 可行域 |
| 约束类型 | 硬约束（动作投影） | 软约束（CBF 条件） | 软约束（代价预算） | 硬约束（可行域划分） |
| 设置 | 在线 / 多智能体 | 在线 / 多智能体 | 在线 / 多智能体 | 离线 / 单智能体 |
| 安全执行点 | env.step() 前投影 | 策略更新时约束 | 策略更新时约束 | 策略学习时加权 BC |
| 多智能体通信 | 无（独立投影） | GNN 图注意力 | 中心化 critic | N/A（单智能体） |
| 动力学要求 | 需要约束雅可比 | 不需要（score function） | 不需要 | 不需要（纯离线） |
| 策略表示 | 高斯 (PPO actor) | 高斯 (PPO actor) | 高斯 (PPO actor) | 扩散模型（多模态） |
| 可行性判断 | 无（仅动作投影） | CBF 值判断 | 无 | V_h* ≤ 0 显式判断 |
| 扩展性 | 受约束维度限制 | GNN 天然可扩展 | 受 critic 输入维度限制 | 受离线数据覆盖度限制 |

---

## 9. Safety-Critical Formation Control (Bohara & Farzan, arXiv 2024)

**链接**: [arxiv:2406.13707](https://arxiv.org/abs/2406.13707) | [HTML](https://arxiv.org/html/2406.13707v2)

### 解决的问题

非完整约束 (non-holonomic) 多机器人系统在**通信受限**环境中的**安全关键编队控制** — 机器人之间无法直接通信，但必须维持编队队形并保证碰撞避免。

现有方法的局限：
1. **需要通信**：大多数编队控制方法依赖智能体间的速度/状态信息交换
2. **全态可知假设**：CBF 方法通常假设能获得邻居的精确速度和方向
3. **非完整约束难处理**：差分驱动等非完整系统不能横向运动，编队控制设计更复杂
4. **干扰传播**：Leader 的速度波动会沿编队链放大（弦不稳定问题）

### 系统模型：前驱-跟随动力学

采用**相对坐标系**（以跟随者为参考系），核心状态变量：

```
d:   前驱与跟随者之间的距离
θ:   前驱相对于跟随者航向的角度
v₁:  前驱线速度,  v:  跟随者线速度
ψ:   航向角差
ω₁:  前驱角速度,  ω:  跟随者角速度
```

**相对运动方程**（极坐标形式）：

```
ḋ  = v₁cos(θ - ψ) - v·cos(θ)
θ̇  = [v·sin(θ) - v₁·sin(θ - ψ)] / d  -  ω
v̇₁ = u₁  (前驱加速度)
v̇  = u   (跟随者加速度)
ψ̇  = ω₁ - ω
```

**投影到跟随者坐标系**（笛卡尔形式）：

```
ḋₓ  = v₁ₓ - v + dᵧ·ω
ḋᵧ  = v₁ᵧ - dₓ·ω
v̇₁ₓ = aₓ + v₁ᵧ·ω
v̇₁ᵧ = aᵧ - v₁ₓ·ω
```

其中 `dₓ = d·cos(θ)`, `dᵧ = d·sin(θ)` 是前驱在跟随者坐标系下的 x/y 分量。

### 编队图结构：有向无环图 (DAG)

编队使用 DAG 描述，两种边类型：

| 边类型 | 含义 | 距离维持方式 |
|--------|------|-------------|
| **X⁺-edge** | 沿跟随者 x 轴方向 | 速度依赖距离：`dₓ* + T·v`（含时间车距 T） |
| **Y-edge** | 沿跟随者 y 轴方向 | 固定距离：`dᵧ*` |

每个跟随者至少需要 1 条 X⁺-edge 和 1 条 Y-edge（出度 ≥ 2）。

### 去中心化状态估计器

**核心挑战**：无通信 → 跟随者无法直接获取前驱的速度和航向 → 必须从相对位置测量中**估计**。

**估计器设计**（x 轴分量）：

```
d̂̇ₓ  = v̂₁ₓ - v + d·ω·sin(θ) + gₐ·d̃ₓ
v̂̇₁ₓ = gᵥ·d̃ₓ + v̂₁ᵧ·ω + p·ω·d̃ᵧ
```

**估计器设计**（y 轴分量）：

```
d̂̇ᵧ  = v̂₁ᵧ - d·ω·cos(θ) + gₐ·d̃ᵧ
v̂̇₁ᵧ = gᵥ·d̃ᵧ - v̂₁ₓ·ω - p·ω·d̃ₓ
```

其中：
- `d̃ₓ = dₓ - d̂ₓ`, `d̃ᵧ = dᵧ - d̂ᵧ` 为位置估计误差
- `gₐ, gᵥ, p` 为可调估计增益

**从估计速度恢复前驱状态**：

```
v̂₁ = √(v̂₁ₓ² + v̂₁ᵧ²)        (估计速度大小)
ψ̂  = arctan(v̂₁ᵧ / v̂₁ₓ)      (估计航向差)
```

**增益选择规则**：

| 场景 | 增益约束 |
|------|---------|
| 常速度 | `gₐ < 0`, `-gₐ²/4 ≤ gᵥ < 0`, `p = ½(gₐ + √(gₐ² + 4gᵥ))` |
| 时变速度 | `p = gₐ/3`, `gᵥ = -2gₐ²/9` |

### 估计器稳定性分析

**情况 1：常速度** — 误差动力学矩阵 A 为 Hurwitz 矩阵 → **全局渐近稳定** (GAS)

**情况 2：时变线速度（ω=ω₁=0）** — **定理 1**：

Lyapunov 函数：

```
V = ½[(r·d̃ₓ - ṽₓ)² + |gᵥ|·d̃ₓ² + ṽₓ²]
```

其中 `r > 1` 满足 `r² + r·gₐ - gᵥ = 0`。

结论：估计误差**全局一致最终有界 (GUUB)**，误差上界与最大加速度成正比、与估计增益成反比：

```
|d̃ₓ| > εₐ  ⟹  V̇ < 0
|r·d̃ₓ - ṽₓ| > εᵥ  ⟹  V̇ < 0
```

**情况 3：一般时变运动（ω≠0）** — **定理 2**：

组合 Lyapunov 函数 `V = V₁ + V₂` 覆盖双轴。在 `r = -2p` 条件下，估计器对一般运动模式保持 GUUB 性质。

### 控制屏障函数 (CBF) 安全约束

**X⁺-edge 安全屏障函数**：

```
h₁ = dₓ - dₛ - T·v ≥ 0
```

- `dₛ`: 最小安全距离
- `T`: 时间车距（time headway）— 越大 → 速度依赖间距越大 → 弦稳定性越好
- `v`: 跟随者速度

**安全集**：`C = {(dₓ, v) ∈ R² : h₁(dₓ, v) ≥ 0}`

**CBF 条件**（确保安全集正向不变）：

```
ḣ₁ ≥ -α(h₁)
```

其中 α 为扩展 class-K 函数。

**展开求控制约束**：

```
ḣ₁ = v₁ₓ - v + dᵧ·ω - T·u ≥ -α(h₁)
⟹ T·u ≤ v₁ₓ - v + dᵧ·ω + α(h₁)
```

代入估计值和误差界 Eᵤ：

```
T·u ≤ v̂₁ₓ - Eᵤ - v + dᵧ·ω + α(h₁)
```

### 闭合形式安全关键控制律

**X⁺-edge 加速度控制**：

```
u = κ · (v̂₁ₓ - Eᵤ - xc - v + dᵧ·ω + α(h₁))
```

其中：
- `κ = 1/T`：时间车距的倒数
- `xc ≥ 0`：保守性余量参数
- `α(h₁)`：屏障函数项保证安全

**该控制律同时实现三个目标**：
1. 碰撞避免：`h₁ ≥ 0` 始终成立
2. 编队跟踪：收敛到期望距离 `dₓ* + T·v`
3. 估计误差补偿：通过 `Eᵤ` 项鲁棒地处理估计不确定性

**Y-edge 控制器**：类似地维持 `h₂ = dᵧ - dᵧ* ≥ 0`。

### 弦稳定性 (String Stability)

**定义**：弦稳定性增益 S 衡量干扰从 leader 沿跟随者链的传播。`S < 1` 表示干扰被衰减。

**关键机制**：时间车距 T 在安全屏障 `h₁ = dₓ - dₛ - T·v` 中起核心作用：

```
T 越大 → 速度相关间距越大 → 对速度扰动的响应越慢 → 干扰被吸收
```

**弦稳定性递推结构**：

```
e_{i+1} = G(s) · e_i     (传递函数)
|G(jω)| < 1, ∀ω         (频域弦稳定条件)
```

仿真结果验证 S ≈ 0.7~0.9（远小于 1），表明编队对 leader 扰动有良好的衰减特性。

### 实验验证

**数值仿真（三个场景）**：

| 场景 | 设置 | 结果 |
|------|------|------|
| 常速 leader | Leader 匀速运动 | 编队 15-20s 内收敛，稳态误差 < 0.1m |
| 加速 leader | Leader 线性加速 | 无碰撞发生，弦稳定增益 S ≈ 0.85 |
| 编队重构 | 运行中改变期望距离 | 平滑过渡，无安全违反 |

**Gazebo 物理仿真（仓库导航场景）**：

| 指标 | 数值 |
|------|------|
| 碰撞避免距离裕度 | 0.15 - 0.25m |
| 编队跟踪误差 | 0.05 - 0.15m |
| 弦稳定增益 | 0.7 - 0.9 |

**估计器性能**：
- 收敛时间与 |gₐ| 成反比
- 最优增益范围 gₐ ≈ -3 ~ -5（平衡速度与振荡）

### 方法特点总结

| 特性 | 本文方法 |
|------|---------|
| 通信需求 | **无**（完全去中心化） |
| 安全机制 | CBF（控制屏障函数） |
| 编队拓扑 | DAG (有向无环图) |
| 运动学模型 | 非完整约束（差分驱动） |
| 安全保证 | h₁ ≥ 0 正向不变性 |
| 稳定性 | GAS (常速) / GUUB (时变) |
| 弦稳定性 | 通过时间车距 T 保证 |
| 闭合形式解 | 有（无需在线优化/QP） |

### 与本课题的关系

本文与 ATACOM 方法在**安全编队控制**层面高度互补，核心差异在于安全机制和动力学假设：

| 方面 | 本文 (Bohara 2024) | ATACOM (本论文) |
|------|-------------------|----------------|
| 安全机制 | CBF（h ≥ 0 正向不变） | 约束流形零空间投影 |
| 安全实现 | 闭合形式控制律 | 雅可比伪逆 + 零空间投影 |
| 通信需求 | 无（状态估计器替代） | 无（独立投影） |
| 动力学 | 非完整约束（差分驱动） | 全向运动（质点模型） |
| 控制层级 | 单层（估计器+控制器） | 双层（安全投影+RL策略） |
| 学习成分 | 无（纯控制方法） | 有（RL训练策略） |
| 弦稳定性 | 显式分析和保证 | 无（未涉及） |
| 约束类型 | 碰撞距离约束 | 通用不等式/等式约束 |
| 可扩展性 | DAG 拓扑，链式传播 | 约束维度增长 |

**四个融合方向**：

1. **估计器 → ATACOM 的邻居感知**：本文的去中心化估计器可直接嵌入 ATACOM 多智能体框架。当前 `mult_cm.py` 假设能直接获取其他智能体状态，而真实场景中通信可能不可靠。将本文的估计器集成到 ATACOM 中，可在**通信受限时仍保持安全约束投影的有效性**。

2. **CBF + ATACOM 双层安全**：本文的 CBF `h₁ = dₓ - dₛ - T·v` 可作为 ATACOM 的约束函数 `k(s)`。这样 ATACOM 的零空间投影不仅保证约束满足，还继承了 CBF 的时间车距特性和弦稳定性保证。

3. **弦稳定性分析 → ATACOM 编队**：本文的弦稳定性框架（S < 1）是 ATACOM 多智能体编队目前缺失的分析工具。将时间车距 T 引入 ATACOM 的碰撞约束，可以在安全保证之上**额外获得干扰衰减特性**。

4. **非完整约束 → ATACOM 环境扩展**：本文处理差分驱动等非完整系统，而 ATACOM 当前在 Safety-Gymnasium 的全向质点模型上验证。将 ATACOM 扩展到非完整系统是重要的实际应用方向 — 本文提供了非完整动力学下编队控制的完整参考。

### 与 DGPPO/GCBF+ 的三角关系

```
CBF 安全方法谱系：

DGPPO (Zhang 2025)           本文 (Bohara 2024)
  · 学习的 CBF (GNN)           · 手工设计的 CBF
  · 离散时间 DCBF               · 连续时间 CBF
  · 多智能体 GNN 通信           · 无通信 (估计器替代)
  · RL 策略 (PPO)               · 闭合形式控制律
  · 全态可控                     · 非完整约束
         ↓                            ↓
    学习安全证书                   经典控制理论
         ↘                          ↙
              ATACOM (本课题)
              · 约束流形投影 (非 CBF)
              · RL 策略 + 安全投影
              · 可融合 CBF 作为约束函数
```

**三者互补**：DGPPO 提供学习的 CBF + GNN 通信；本文提供无通信估计 + 弦稳定性；ATACOM 提供流形投影的硬约束保证。融合三者可实现**可学习、无通信依赖、流形约束保证、弦稳定**的安全多智能体编队系统。

---

## 10. Solving the Right Problem with Multi-Robot Formations (Cornwall & Bos, arXiv 2025)

**链接**: [arxiv:2510.25422](https://arxiv.org/abs/2510.25422) | [HTML](https://arxiv.org/html/2510.25422)

### 核心问题：编队形状 ≠ 任务目标

传统编队控制存在根本性的**目标不匹配 (mismatch)** — 维持编队形状 ≠ 最小化真实任务代价函数：

```
任务目标（如保护载荷、避障）
    ↓ 人工抽象
预设编队形状（Diamond, Box, Wedge 等）
    ↓ 编队控制器
机器人位置
```

- 美军步兵条令规定 7 种预设编队 (Column, Line, Wedge, Echelon, Vee, Diamond, Box)
- 这些形状是人类经验的启发式编码，不能在所有环境下最小化实际代价
- 当环境信息可用时，静态形状往往远非最优

**理想编队** vs **实际操作**：

```
理想:   X*[t+1] = arg min_X[t] Σ_j r*_j[t]     (真实代价最优)
实际:   X̂*[t+1] ≈ arg min_X[t] Σ_j r̂_j[t]     (代理代价近似)
```

本文目标：**缩小 r̂ 与 r* 之间的差距**，同时保留编队控制器的效率优势。

### 方法框架：编队规划器 + 编队控制器

```
┌──────────────────────────────────────────────────┐
│              编队规划器 (Formation Planner)         │
│                                                    │
│  Step 1: 自适应权重估计                              │
│    · 观察真实代价 c*(x)                             │
│    · 约束最小二乘求解最优权重 α                       │
│    · rank-one 增量更新（无需存储历史数据）             │
│                                                    │
│  Step 2: 代理代价最小化                              │
│    · 用加权代理函数 ĉ(x) = Σ αᵢfᵢ(x) 近似真实代价    │
│    · 数值优化求解最优相对位置                          │
│    · 输出：期望编队构型 d̂                             │
├──────────────────────────────────────────────────┤
│              编队控制器 (Formation Controller)       │
│                                                    │
│  · 接收期望相对位置 d̂                               │
│  · Lyapunov 非合作控制器                             │
│  · 驱动智能体到规划器输出的构型                        │
│  · 仅需局部相对位置信息                               │
└──────────────────────────────────────────────────┘
```

### 代理代价函数体系

**真实代价函数**（不可微/不连续，用于评估）：

| 代价 | 公式 | 含义 |
|------|------|------|
| 保护代价 c₁ | `Σᵢ minⱼ los(sᵗᵢ - xᵃⱼ, sᵖ - xᵃⱼ)` | 智能体未能遮挡威胁到载荷视线的程度 |
| 避障代价 c₂ | `Σₖ Σⱼ ‖sᵒₖ - xᵃⱼ‖⁻¹` | 与障碍物距离的倒数 |
| 违反代价 c₃ | `Σᵢ[minⱼ los(…) > τₚ] + Σₖ Σⱼ[‖sᵒₖ - xᵃⱼ‖ < τₒ]` | 二值违反计数（不连续） |

**代理基函数**（连续可微，用于优化）：

| # | 基函数 | 公式 | 物理含义 |
|---|--------|------|---------|
| f₁ | 接近性 | `Σᵢ ‖sᵖ - xᵃᵢ‖²` | 鼓励智能体靠近载荷 |
| f₂ | 保护性 | `Σᵢ Σⱼ∈Nᵢ los(sᵗⱼ - xᵃᵢ, sᵖ - xᵃᵢ)` | 视线遮挡质量 |
| f₃ | 避障 | `Σᵢ Σₖ∈Nᵢ exp(-ζ‖sᵒₖ - xᵃᵢ‖²)` | 高斯衰减的障碍排斥 |
| f₄ | 智能体互避 | `Σᵢ Σᵢ'∈Nᵢ exp(-ζ‖xᵃᵢ' - xᵃᵢ‖²)` | 智能体间排斥 |
| f₅ | 载荷避让 | `Σᵢ exp(-ζ‖sᵖ - xᵃᵢ‖²)` | 避免撞到被保护对象 |

**代理代价的线性组合**：`ĉ(x) = Σᵢ αᵢ · fᵢ(x) = α^T f(x)`

### 自适应权重估计

**约束最小二乘问题**：

```
minimize   ‖Fₜ(x) · α - c*ₜ(x)‖²_W
subject to  Σᵢ αᵢ = 1,  α > 0
```

其中 `Fₜ(x) ∈ ℝ^{Mₜ×N}` 为历史基函数评估矩阵，`W` 为指数衰减对角加权矩阵。

**高效增量更新（rank-one update）**：

```
Rₜ = wₜ · (Rₜ₋₁ + fₘₜ · fₘₜ^T)
pₜ = wₜ · (pₜ₋₁ + fₘₜ · c*ₘₜ)
yₜ = wₜ · (yₜ₋₁ + c*ₘₜ · c*ₘₜ)
```

每步仅需 O(N²) 计算（N=5 个基函数）。

**定理 1（自适应权重的局限性）**：改变权重总能改变线性组合的最小值，**当且仅当**基函数梯度集 `{∇ₓfᵢ(x)}` 线性无关：

```
∃(α, β): x̂_α = x̂_β, α ≠ γβ  ⟺  det(A) = 0
其中 A = [∇ₓf₀(x)  ∇ₓf₁(x)  ...  ∇ₓf_{N-1}(x)]
```

| 场景 | det(A) | 自适应权重效果 |
|------|--------|---------------|
| 多个代价函数梯度方向不同 | ≠ 0 | 有效 — 权重调整可改变最优解 |
| 多个代价函数梯度近似平行 | ≈ 0 | 无效 — 不同权重得到相同最优解 |
| 远离障碍物（避障梯度为零） | = 0 | 无效 — 固定权重即可 |

### 编队控制器设计（Lyapunov 方法）

误差定义：`eˣᵢⱼ = dˣᵢⱼ - d̂ˣᵢⱼ`（智能体间距离误差），`eˢᵢⱼ = dˢᵢⱼ - d̂ˢᵢⱼ`（智能体-环境实体距离误差）

Lyapunov 函数：

```
V(X, S) = ½ Σᵢ [Σⱼ∈Nˣᵢ (eˣᵢⱼ)^T eˣᵢⱼ  +  Σⱼ∈Nˢᵢ (eˢᵢⱼ)^T eˢᵢⱼ]
```

控制律通过关联矩阵 B 确保 `V̇(X, S) ≤ 0`：

```
B₁^T · u = d - d̂ - B₂^T · ṡ
```

特性：非合作（仅需局部相对位置信息）、Lyapunov 稳定、无需集中通信。

### 实验结果

**实验设置**：Robotarium 仿真器，3.2m×2.0m，4 个随机环境 × 5 次试验/方法 = 80 次，每次 3000 步 (~100s)。1 个 leader + 3-4 个保护智能体 + 2-3 个威胁 + 3-4 个障碍物。

**对比方法**：

| 方法 | 编队规划 | 控制器 | 避障方式 |
|------|---------|--------|---------|
| **FP-AW** | 数值优化 + 自适应权重 | Lyapunov | 规划器内优化 |
| **FP** | 数值优化 + 静态权重 | Lyapunov | 规划器内优化 |
| **FS** | 人工选择 (Box) | Lyapunov | 屏障证书 |
| **AF** | 人工选择 (仿射) | 仿射控制 | 屏障证书 |

**单目标（保护代价）**：

| 方法 | Env 1 | Env 2 | Env 3 | Env 4 | 总分 |
|------|-------|-------|-------|-------|------|
| FP-AW | 1.00 | 1.80 | 1.18 | 1.04 | 5.02 |
| FP | 1.22 | 1.00 | 1.00 | 1.00 | **4.22** |
| FS | 3.59 | 4.21 | 4.21 | 4.26 | 15.18 |
| AF | 5.69 | 11.40 | 11.40 | 4.85 | 29.71 |

编队规划方法比静态形状降低约 **75%** 保护代价，比仿射控制降低约 **85%**。

**多目标（保护 + 避障 + 违反）**：

| 方法 | 总分 |
|------|------|
| **FP-AW** | **27.00** |
| AF | 40.23 |
| FS | 46.36 |
| FP | 82.98 |

多目标时 FP-AW 显著优于所有其他方法（降低 35-67%），自适应权重的价值在多目标场景中才真正体现。

```
单目标时:  FP ≈ FP-AW >> FS > AF   (自适应权重无额外优势，验证定理 1)
多目标时:  FP-AW >> AF > FS >> FP   (自适应权重显著优于固定权重)
```

### 方法特点总结

| 特性 | 本文方法 |
|------|---------|
| 编队规划 | 代理代价连续优化（非预设形状） |
| 权重调整 | 数据驱动自适应（rank-one 增量更新） |
| 控制器 | Lyapunov 非合作控制器 |
| 通信需求 | 仅需局部相对位置 |
| 理论贡献 | 定理 1 — 自适应权重有效性的充要条件 |
| 局限 | 环境完全已知、静态障碍物/威胁、仅测试 3-4 个智能体 |

### 与本课题的关系

本文与 ATACOM 工作在**不同层面**，天然互补：

| 方面 | 本文 (Cornwall 2025) | ATACOM (本课题) |
|------|---------------------|----------------|
| 核心贡献 | 编队规划器（代理代价优化） | 安全动作投影（约束流形） |
| 解决层面 | **"编什么队形"** | **"如何安全执行"** |
| 安全机制 | 代理代价中隐含（f₃ 避障项） | 约束雅可比零空间投影（硬保证） |
| 优化方式 | 数值优化代理代价 | 解析投影（伪逆+零空间） |
| 学习成分 | 自适应权重（监督学习） | RL 策略（PPO actor） |
| 编队维持 | Lyapunov 控制器 | 约束函数 |
| 多目标处理 | 加权代理代价线性组合 | 多约束联合雅可比 |

**层次互补架构**：

```
┌─────────────────────────────────┐
│  任务层: "做什么?"                │
│  · 保护载荷、避障、最优覆盖       │
└──────────────┬──────────────────┘
               │
┌──────────────▼──────────────────┐
│  编队规划层: "编什么队形?"         │  ← 本文 (Cornwall 2025)
│  · 代理代价函数优化               │    FP-AW 自适应权重
│  · 输出: 期望相对位置 d̂           │
└──────────────┬──────────────────┘
               │
┌──────────────▼──────────────────┐
│  安全执行层: "如何安全到达?"       │  ← ATACOM (本课题)
│  · 约束流形投影                   │    零空间安全动作
│  · 碰撞避免硬保证                 │
│  · RL 策略学习导航行为            │
└─────────────────────────────────┘
```

**五个融合方向**：

1. **代理代价 → ATACOM 约束函数**：本文的基函数可直接转化为 ATACOM 约束（如 `f₃` 避障 → `k(s) = d_min - ‖sᵒₖ - xᵃᵢ‖ ≤ 0`）。代理代价优化提供**期望构型**，ATACOM 约束投影提供**安全保证**。

2. **自适应权重 → ATACOM 多约束优先级**：当多个约束接近激活时，本文的数据驱动权重估计可为 ATACOM 的约束优先级提供动态调整。

3. **定理 1 → ATACOM 约束冗余分析**：当多个约束的雅可比行向量线性相关（约束冗余）时，零空间维度不减少。定理 1 为 ATACOM 的**约束集简化**提供了理论判据。

4. **编队规划器作为 ATACOM 的高层目标生成器**：用 FP-AW **动态生成最优 d***，使编队不仅安全还最小化任务代价：`固定 d* → FP-AW → d̂* → 约束 k(s) = ‖xᵢ - xⱼ‖ - d̂* ≤ 0`

5. **Lyapunov 控制器 → ATACOM 的确定性备份**：本文的 Lyapunov 控制器（无需学习、解析稳定）可作为 RL 策略异常时的安全备份。

### 与编队控制参考文献的关系

```
编队控制方法关系图:

Cornwall 2025 (本文)              Bohara 2024
  · 编队规划器                      · 无通信 CBF 编队
  · 代理代价优化                    · 弦稳定性
  · 自适应权重                      · 非完整约束
       ↘                         ↙
          编队目标生成         安全保证
               ↘               ↙
              ATACOM (本课题)
              · 约束流形投影
              · RL 安全策略学习
               ↗               ↖
          团队协调           硬约束保证
       ↗                         ↖
AFOR (Deng 2025)              DGPPO (Zhang 2025)
  · GNN 分层编队                · 图 CBF
  · 弹簧-阻尼器                 · 学习的屏障函数
  · 窄通道自适应                 · 分布式安全
```

**本文独特贡献**：首次系统性量化了"编队形状"与"任务代价"之间的 mismatch，并提供了可计算的缩减方案。这一"解决正确的问题"的思想启发所有编队方法 — 不应仅关注"如何保持编队"，更应关注"保持什么样的编队才最优"。

---

## 11. Safe MARL for Formation Control without Individual Reference Targets (Dawood et al., IEEE RA-L 2025)

**链接**: [arxiv:2312.12861](https://arxiv.org/abs/2312.12861) | [HTML](https://arxiv.org/html/2312.12861v2)

### 核心问题

首次提出**无个体参考目标**的安全编队控制 — 仅指定编队质心目标位置，不为每个机器人分配参考位置。机器人必须自组织地维持编队，同时在训练和执行全程保证零碰撞。

### 方法框架

**CTDE + 注意力 Critic + MPC 安全滤波器**：

```
训练: MASAC (off-policy) + 注意力 Critic（建模智能体交互）
执行: 独立 Actor（仅用局部观测：LiDAR + 2邻居 + 质心信息）
安全: MPC 安全滤波器（每步优化 min ‖a - a_RL‖² s.t. 预测轨迹无碰撞）
```

**MPC 与 ATACOM 的关键对比**：
- MPC: 预测性安全（看未来 T 步），每步求解优化，计算贵
- ATACOM: 解析投影（伪逆+零空间），仅约束当前步，计算快

### 实验结果

| 指标 | ATT_MPC (本文) | MASAC | MADDPG |
|------|-------------|-------|--------|
| 成功率（无障碍） | **99.5%** | 58.0% | 9.5% |
| 碰撞率 | **0%** | 8.9% | 57.7% |
| S形路径完成率 | **100%** | 75.8% | — |
| 可扩展性 | 3→8 机器人零碰撞 | — | — |
| 真实机器人 | 零碰撞 | 有碰撞 | — |

MPC 加速训练 4 倍（消除碰撞 → 有效探索增加）。去掉 MPC 后碰撞率仅 1.4%，表明智能体部分内化了安全行为。

### 与本课题的关系

| 方面 | 本文 | ATACOM |
|------|------|--------|
| 安全机制 | MPC 安全滤波器 | 约束流形零空间投影 |
| 安全实现 | 在线优化 | 解析投影 |
| 学习算法 | SAC (off-policy) | PPO (on-policy) |
| 编队目标 | 仅质心目标 | 个体间距离约束 |

**五个融合方向**：(1) MPC预测+ATACOM解析=双层安全；(2) "无个体目标"思想简化ATACOM编队约束设计；(3) 注意力Critic替换ATACOM的MLP Critic；(4) "仅2邻居"观测实现可变规模编队；(5) MPC偏差惩罚启发ATACOM约束违反惩罚。

---

## 12. Exploiting Geometric Constraints in MAPF (Atzmon et al., ICAPS 2023)

**链接**: [ICAPS](https://ojs.aaai.org/index.php/ICAPS/article/view/27174)

### 核心问题

利用几何约束加速多智能体路径规划 — 每个智能体走最短路径 + 在出发前添加"安全延迟" τᵢ，仅用距离信息保证无冲突，无需路径搜索。

### 关键定理

**定理 5（充要条件）**：安全延迟差集合由 Ψᵢⱼ 和 Λᵢⱼ 完全决定：

```
Ψᵢⱼ = d(sᵢ,sⱼ) + d(gᵢ,gⱼ) - d(sᵢ,gᵢ) - d(sⱼ,gⱼ)

Ψᵢⱼ > 0  → 任何延迟差都安全（路径不可能交叉）
Ψᵢⱼ ≤ 0  → τⱼ - τᵢ 不能落在 [-Λⱼᵢ, Λᵢⱼ] 区间内
```

最小化总延迟是 **NP-complete**（等价于单机调度问题）。

### DSP 算法

| 特性 | DSP |
|------|-----|
| 复杂度 | O(k²) |
| 搜索方式 | **无图搜索** — 仅用距离 |
| 可扩展性 | **10000+ 智能体** |
| 运行时间 | **< 100ms** |
| 最佳排序 | LH (长路径优先) 或 LD (贪心最小延迟) |

密集环境中 DSP 接近甚至超越 PP（Prioritized Planning），运行时间快 3-4 个数量级。

### 与本课题的关系

| 方面 | 本文 | ATACOM |
|------|------|--------|
| 领域 | 离散路径规划 | 连续安全 RL |
| 安全机制 | 几何约束+安全延迟 | 约束流形投影 |
| 智能体规模 | **10000+** | 数个到数十个 |

**三个启发方向**：(1) Ψᵢⱼ判据用于ATACOM约束激活预筛，减少约束雅可比维度；(2) 优先级+延迟思想用于分层约束投影；(3) 距离轮廓三角不等式为编队约束可行性检查提供理论工具。

---

## 13. Safe RL for Robotics: From Exploration to Policy Learning (Puze Liu, TU Darmstadt 博士论文, 2024)

**独立学习笔记**: [ATACOM-Thesis-Liu2024-学习笔记.md](ATACOM-Thesis-Liu2024-学习笔记.md)

### 解决什么问题

**总问题**：RL需要"试错"来学习，但真实机器人试错的代价不可接受。现有Safe RL方法（PPO-Lag、CPO、WCSAC）仅在损失函数中加惩罚项，只保证收敛后策略大致安全，**无法保证训练过程中每一步安全**。

**核心研究问题**：领域知识的多少与安全保障级别之间存在什么权衡关系？

论文沿"领域知识从多到少"的谱系，系统性地提出三组方法：

| 章节 | 子问题 | 已有方法不足 | 提出的方法 |
|------|--------|-----------|----------|
| Ch2 | 已知动力学+约束时，能否保证训练中**每步安全**？ | CBF-QP多约束可能不可行；投影改变RL动作分布 | **ATACOM** — 切空间投影，结构安全 |
| Ch3 | 约束太复杂无法手工设计；当前步安全≠**长期安全** | 球体近似太粗糙；距离约束不考虑未来 | **ReDSDF**（学习距离场）+ **D-ATACOM**（FVF+CVaR） |
| Ch4 | 连动力学模型都没有时怎么办？ | 值函数安全评价：多约束尺度不一、阈值无语义、可行域是安全集超集 | **SPF/DSPF** — 安全概率Bellman方程 + SUKB |

### ATACOM核心机制（Ch2）

将约束 $k(s) \leq 0$ 通过松弛变量转化为等式约束 $c(s,\mu) = k(s) + \alpha(\mu) = 0$，定义约束流形 $\mathcal{M}$，RL策略**直接在流形切空间中输出动作**：

$$[u_s; u_\mu] = \underbrace{-J_u^\dagger\psi}_{\text{漂移补偿}} \underbrace{-\lambda J_u^\dagger c}_{\text{收缩}} + \underbrace{B_u \cdot u}_{\text{RL动作（切空间中）}}$$

安全性证明：$V = \frac{1}{2}c^Tc$，$\dot{V} = -\lambda c^T J_u J_u^\dagger c \leq 0$（LaSalle不变性原理）。ISS鲁棒性：有界扰动下 $\|c\| \leq \eta_c/\lambda$。

与CBF关键区别：CBF是"先输出再投影修正"，ATACOM是"直接在安全空间中输出"→ 不改变策略分布，与RL天然兼容。

### D-ATACOM长期安全（Ch3）

Feasibility Value Function：$V_F^\pi(s) = \mathbb{E}[\sum \gamma^t \max(k(s_t), 0)]$。用分布式RL + CVaR风险约束实现长期安全与不确定性处理。Navigation任务中D-ATACOM是**唯一**展现主动避障行为的方法。

### SPF无模型安全（Ch4）

安全概率Bellman方程：$\Psi^\pi(s) = (1-\gamma)p_b(s) + \gamma p_b(s)\mathbb{E}_{s'}[\Psi^\pi(s')]$。输出 $[0,1]$ 的安全概率（vs 值函数的无界代价），可行域 $\subseteq$ 安全集（更严格）。DSPF通过Beta分布 + Prior Network捕获认知不确定性，SUKB引导乐观探索。

### 与本课题的关系

**直接理论基础**：`safeRL_manifold` 的 `constrained_manifold/manifold.py`（AtacomEnvWrapper）直接实现Ch2 ATACOM算法。

**硕士课题的核心创新方向**：论文全部是单智能体，课题扩展到多智能体：
- 分布式ATACOM：每个智能体维护局部约束流形
- 层次化安全：RMPflow层次结构 + ATACOM安全层
- 多智能体D-ATACOM/SPF：联合FVF或安全概率考虑其他智能体行为
- 编队+ATACOM：编队保持（等式约束）+ 碰撞避免（不等式约束）
