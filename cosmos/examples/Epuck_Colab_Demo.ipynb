{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-puck Multi-Robot Simulation Demo\n",
    "\n",
    "This notebook demonstrates the E-puck multi-robot simulation with visualization.\n",
    "\n",
    "**Features:**\n",
    "- Multi-robot formation control\n",
    "- Safety constraint visualization\n",
    "- Animated trajectory display\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ustbmicl-ros2epucksRL/safe-rl-manifold-suite/blob/master/Epuck_Colab_Demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/ustbmicl-ros2epucksRL/safe-rl-manifold-suite.git\n",
    "%cd safe-rl-manifold-suite\n",
    "\n",
    "# Install dependencies\n",
    "!pip install torch numpy matplotlib gymnasium hydra-core omegaconf -q\n",
    "!pip install -e . -q\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display, Image\n",
    "\n",
    "# Import COSMOS\n",
    "from cosmos.envs.webots_wrapper import EpuckSimEnv\n",
    "from cosmos.envs.epuck_visualizer import EpuckVisualizer, run_episode_with_visualization\n",
    "from cosmos.algos.mappo import MAPPO\n",
    "from cosmos.safety.cosmos_filter import CBFFilter\n",
    "\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create E-puck environment\n",
    "env = EpuckSimEnv(\n",
    "    num_agents=4,\n",
    "    arena_size=1.0,\n",
    "    max_steps=200,\n",
    "    dt=0.064\n",
    ")\n",
    "\n",
    "print(f\"Environment created!\")\n",
    "print(f\"  - Number of agents: {env.num_agents}\")\n",
    "print(f\"  - Observation dim: {env.get_obs_dim()}\")\n",
    "print(f\"  - Action dim: {env.get_act_dim()}\")\n",
    "print(f\"  - Arena size: {env._arena_size}m x {env._arena_size}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Single Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "obs, share_obs, info = env.reset(seed=42)\n",
    "\n",
    "# Create visualizer\n",
    "vis = EpuckVisualizer(env, show_sensors=True, show_goals=True)\n",
    "\n",
    "# Render initial state\n",
    "plt.figure(figsize=(8, 8))\n",
    "vis.render(env)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Episode with Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run episode with random actions\n",
    "result = run_episode_with_visualization(\n",
    "    env,\n",
    "    policy=None,  # Random policy\n",
    "    max_steps=100,\n",
    "    render_interval=10,\n",
    "    save_animation=True,\n",
    "    output_path=\"random_policy.gif\"\n",
    ")\n",
    "\n",
    "print(f\"\\nEpisode finished!\")\n",
    "print(f\"  - Total reward: {result['total_reward']:.2f}\")\n",
    "print(f\"  - Total cost: {result['total_cost']:.2f}\")\n",
    "print(f\"  - Steps: {result['steps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display animation\n",
    "Image(filename=\"random_policy.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run with CBF Safety Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CBF safety filter\n",
    "cbf = CBFFilter(\n",
    "    env_cfg={'arena_size': env._arena_size, 'num_agents': env.num_agents},\n",
    "    safety_cfg=None\n",
    ")\n",
    "\n",
    "def safe_random_policy(obs):\n",
    "    \"\"\"Random policy with CBF safety filtering.\"\"\"\n",
    "    # Generate random actions\n",
    "    actions = np.random.uniform(-0.8, 0.8, (env.num_agents, env.get_act_dim()))\n",
    "    \n",
    "    # Apply CBF safety filter\n",
    "    constraint_info = env.get_constraint_info()\n",
    "    safe_actions = cbf.project(actions, constraint_info)\n",
    "    \n",
    "    return safe_actions, None\n",
    "\n",
    "# Reset environment\n",
    "env.reset(seed=123)\n",
    "\n",
    "# Run with safety filter\n",
    "result_safe = run_episode_with_visualization(\n",
    "    env,\n",
    "    policy=safe_random_policy,\n",
    "    max_steps=150,\n",
    "    render_interval=10,\n",
    "    save_animation=True,\n",
    "    output_path=\"safe_policy.gif\"\n",
    ")\n",
    "\n",
    "print(f\"\\nSafe episode finished!\")\n",
    "print(f\"  - Total reward: {result_safe['total_reward']:.2f}\")\n",
    "print(f\"  - Total cost (collisions): {result_safe['total_cost']:.2f}\")\n",
    "print(f\"  - Steps: {result_safe['steps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display safe animation\n",
    "Image(filename=\"safe_policy.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train MAPPO Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MAPPO algorithm\n",
    "mappo = MAPPO(\n",
    "    obs_dim=env.get_obs_dim(),\n",
    "    share_obs_dim=env.get_share_obs_dim(),\n",
    "    act_dim=env.get_act_dim(),\n",
    "    num_agents=env.num_agents,\n",
    "    cfg={\n",
    "        'actor_lr': 3e-4,\n",
    "        'critic_lr': 3e-4,\n",
    "        'gamma': 0.99,\n",
    "        'gae_lambda': 0.95,\n",
    "        'clip_param': 0.2,\n",
    "        'ppo_epochs': 10,\n",
    "        'num_mini_batch': 4,\n",
    "    },\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "print(\"MAPPO created!\")\n",
    "print(f\"  - Actor parameters: {sum(p.numel() for p in mappo.actor.parameters())}\")\n",
    "print(f\"  - Critic parameters: {sum(p.numel() for p in mappo.critic.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training loop\n",
    "from cosmos.buffers import RolloutBuffer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Create buffer\n",
    "buffer = RolloutBuffer(\n",
    "    episode_length=200,\n",
    "    num_agents=env.num_agents,\n",
    "    obs_dim=env.get_obs_dim(),\n",
    "    share_obs_dim=env.get_share_obs_dim(),\n",
    "    act_dim=env.get_act_dim(),\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95\n",
    ")\n",
    "\n",
    "# Training\n",
    "num_episodes = 50\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in tqdm(range(num_episodes), desc=\"Training\"):\n",
    "    obs, share_obs, _ = env.reset(seed=episode)\n",
    "    buffer.set_first_obs(obs, share_obs)\n",
    "    \n",
    "    ep_reward = 0\n",
    "    \n",
    "    for step in range(200):\n",
    "        # Get actions from policy\n",
    "        actions, log_probs = mappo.get_actions(obs)\n",
    "        values = mappo.get_values(share_obs)\n",
    "        \n",
    "        # Apply CBF safety filter\n",
    "        constraint_info = env.get_constraint_info()\n",
    "        safe_actions = cbf.project(actions, constraint_info)\n",
    "        \n",
    "        # Step environment\n",
    "        next_obs, next_share, rewards, costs, dones, infos, truncated = env.step(safe_actions)\n",
    "        \n",
    "        # Store in buffer\n",
    "        masks = (~dones).astype(np.float32).reshape(-1, 1)\n",
    "        buffer.insert(next_obs, next_share, actions, log_probs, values, \n",
    "                     rewards, costs, masks)\n",
    "        \n",
    "        ep_reward += rewards.sum()\n",
    "        obs, share_obs = next_obs, next_share\n",
    "        \n",
    "        if dones.all() or truncated:\n",
    "            break\n",
    "    \n",
    "    # Update policy\n",
    "    last_values = mappo.get_values(share_obs)\n",
    "    buffer.compute_returns_and_advantages(last_values)\n",
    "    mappo.update(buffer)\n",
    "    buffer.after_update()\n",
    "    \n",
    "    episode_rewards.append(ep_reward)\n",
    "    \n",
    "    if (episode + 1) % 10 == 0:\n",
    "        print(f\"Episode {episode+1}: Reward = {ep_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Training Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_policy(obs):\n",
    "    \"\"\"Trained MAPPO policy with CBF safety.\"\"\"\n",
    "    actions, log_probs = mappo.get_actions(obs, deterministic=True)\n",
    "    constraint_info = env.get_constraint_info()\n",
    "    safe_actions = cbf.project(actions, constraint_info)\n",
    "    return safe_actions, log_probs\n",
    "\n",
    "# Reset environment\n",
    "env.reset(seed=999)\n",
    "\n",
    "# Run evaluation\n",
    "result_trained = run_episode_with_visualization(\n",
    "    env,\n",
    "    policy=trained_policy,\n",
    "    max_steps=200,\n",
    "    render_interval=10,\n",
    "    save_animation=True,\n",
    "    output_path=\"trained_policy.gif\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTrained policy evaluation:\")\n",
    "print(f\"  - Total reward: {result_trained['total_reward']:.2f}\")\n",
    "print(f\"  - Total cost: {result_trained['total_cost']:.2f}\")\n",
    "print(f\"  - Steps: {result_trained['steps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display trained policy animation\n",
    "Image(filename=\"trained_policy.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Policy': ['Random', 'Random + CBF', 'MAPPO + CBF'],\n",
    "    'Total Reward': [\n",
    "        result['total_reward'],\n",
    "        result_safe['total_reward'],\n",
    "        result_trained['total_reward']\n",
    "    ],\n",
    "    'Collisions': [\n",
    "        result['total_cost'],\n",
    "        result_safe['total_cost'],\n",
    "        result_trained['total_cost']\n",
    "    ],\n",
    "    'Steps': [\n",
    "        result['steps'],\n",
    "        result_safe['steps'],\n",
    "        result_trained['steps']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Results Comparison\")\n",
    "print(\"=\"*50)\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "print(\"Environment closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
